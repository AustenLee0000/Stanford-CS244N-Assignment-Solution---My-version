{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Written Questions for Assignment 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### (a)\n",
    "\n",
    "$$- \\displaystyle\\sum_{w \\in Vocab}~y_wlog(\\hat{y_w})= - \\displaystyle\\sum_{w \\in Vocab~and~y_w=1}~y_wlog(\\hat{y_w})=-log(\\hat{y_o})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### (2) negative-sampling loss function\n",
    "\n",
    "negative-sampling方法在corpus中随机选取非前后文窗口中的词作为负样例来计算损失函数。这种方法的优点是它不需要损失函数在corpus中每一个词上的损失，对计算能力的要求会大大降低。\n",
    "\n",
    "negative-sampling的损失函数是，\n",
    "\n",
    "$$J_{neg-sampling}(v_c, o, U)=-log(\\sigma(u_o^Tv_c))-\\displaystyle\\sum^{K}_{k=1}log(\\sigma(-u_k^Tv_c))$$\n",
    "\n",
    "其实，$\\sigma(x)=\\frac{1}{1+e^{-x}}$\n",
    "\n",
    "为实践基于这一损失函数的模型训练，我计算出了$J_{neg_sampling}$对于$v_c$和$U$的偏导数，\n",
    "\n",
    "$$\\frac{\\partial J_{neg-sampling}(v_c, o, U)}{\\partial v_c}=- \\frac{u_o}{e^{u_o^Tv_c}+1}+\\displaystyle\\sum^{K}_{k=1} \\frac{u_k}{e^{-u_o^Tv_c}+1}$$\n",
    "\n",
    "$$\\left. \\frac{\\partial J_{neg-sampling}(v_c, o, U)}{\\partial u}\\right|_{u=u_o} =-\\frac{v_c}{e^{u_o^Tv_c}+1}$$\n",
    "\n",
    "$$\\left. \\frac{\\partial J_{neg-sampling}(v_c, o, U)}{\\partial u}\\right|_{u\\neq u_o} = \\displaystyle\\sum^{K}_{k=1}  \\frac{v_c}{e^{-u_o^Tv_c}+1}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### (1) naive-softmax loss function\n",
    "\n",
    "naive-softamax的损失函数是，\n",
    "\n",
    "$$J_{naive-softmax}(v_c, o, U)=-logP(O=o|C=c)$$\n",
    "\n",
    "\n",
    "且证明可得，naive-softmax损失函数得到的损失值和cross-entropy损失函数得到的损失值是一样的，\n",
    "\n",
    "$$cross~entropy=- \\sum_{w\\in Vocab}^{} y_wlog(\\hat{y_w}) = -log(\\hat{y_w}) = -logP(O=o|C=c)$$\n",
    "\n",
    "为实践基于这一损失函数的模型训练，我计算出了$J_{naive-softmax}$对于$v_c$和$U$的偏导数，\n",
    "\n",
    "$$\\frac{\\partial J_{naive-softmax}(v_c, o, U)}{\\partial v_c}=-u_o^T +\\sum_{w\\in Vocab}P(O=o|C=c) u_w$$\n",
    "\n",
    "$$\\left. \\frac{\\partial J_{naive-softmax}(v_c, o, U)}{\\partial u}\\right|_{u=u_o} =v_c +P(O=o|C=c)v_c $$\n",
    "\n",
    "$$\\left. \\frac{\\partial J_{naive-softmax}(v_c, o, U)}{\\partial u}\\right|_{u\\neq u_o} = \\sum_{w\\in Vocab~and~w\\neq o} P(O=o|C=c)v_c $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### skip-gram算法\n",
    "\n",
    "假设中心词是$c=w_t$，前后文窗口是$[w_{t-m}, \\dots, w_{t-1},w_t,w_{t+1},\\dots,w_{t+m}]$(`window_size`=m)。skip-gram对总损失的计算公式是，\n",
    "\n",
    "$$J_{skip-gram}(v_c, o, U)=\\displaystyle\\sum_{-m\\leq j \\leq m~j\\neq t} J(v_c, w_{t+j}, U)$$\n",
    "\n",
    "等式右侧的$J(v_c, o, U)$可以是$J_{neg-sampling}(v_c, o, U)$，也可以是$J_{navie-softmax}(v_c, o, U)$。\n",
    "\n",
    "因此，skip-gram损失函数对于$v_c$和$U$的偏导数是，\n",
    "\n",
    "$$\\frac{\\partial J_{skip-gram}(v_c, w_{t+j}, U)}{\\partial U} = \\displaystyle\\sum_{-m\\leq j \\leq m~j\\neq t} \\frac{\\partial J(v_c, w_{t+j}, U)}{\\partial U}$$\n",
    "\n",
    "$$\\frac{\\partial J_{skip-gram}(v_c, w_{t+j}, U)}{\\partial v_c} = \\displaystyle\\sum_{-m\\leq j \\leq m~j\\neq t} \\frac{\\partial J(v_c, w_{t+j}, U)}{\\partial v_c}$$\n",
    "\n",
    "$$\\left. \\frac{\\partial J_{skip-gram}(v_c, o, U)}{\\partial v_w}\\right|_{w\\neq c} =0$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 使用SGD训练模型\n",
    "\n",
    "SGD的算法可描述为，\n",
    "\n",
    "while J > threshold:\n",
    "<br>&emsp;&emsp;window = sample_window(corpus)\n",
    "<br>&emsp;&emsp;theta_gradient = evaluate_gradient(J, window, theta)\n",
    "<br>&emsp;&emsp;theta = theta - alpha*theta_gradient\n",
    "\n",
    "结合以上全部计算结果，即可对word2vec(skip-gram)模型进行训练。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 代码实践\n",
    "\n",
    "对于该算法的代码实践详见 [word2vec code](files/HW2的副本)。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Neural Transition-Based Dependency Parser模型的实践（还未全部完成）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dependency Parser是分析确定句子中头词（head）与其他词语之间的关系的模型。一个 Neural Transition-Based Dependency Parser模型有两部分组成：\n",
    "1. Parser: 根据模型的判断建立词语之间的dependices，从而确定一个句子的dependency tree。\n",
    "2. Neural Network: 通过大量数据训练从而作出词语关系的判断的神经网络。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Parser\n",
    "\n",
    "此模型中使用的方法是transitioned-based parser，因此在每一步都会建立一个新的parse。整个模型的结构分为三块，分别是：\n",
    "1. 一个stack：储存正在被处理的词语\n",
    "2. 一个buffer： 储存未来要被处理的词语\n",
    "3. 一个dependencies list： 记录模型预测的dependencies。\n",
    "\n",
    "在初始状态下，stack中只有ROOT一个元素，而buffer存储了需要被parse的句子中的所有词语（根据在句子中的顺序），dependencies中此时没有任何内容。\n",
    "\n",
    "Parse的每一步称为partial pars，可能是以下三种操作中的一种：\n",
    "\n",
    "START: $\\sigma=[ROOT],~\\beta=w_1,\\dots,w_n,~A=\\emptyset$\n",
    "\n",
    "1. `SHIFT`: 将buffer中的第一个词移位到stack中，$\\sigma~~w_i|\\beta~~A\\Rightarrow \\sigma|w_i~~\\beta~~A$\n",
    "2. `LEFT-ARC`: 将stack中第二新加的词语标记为第一新加的词语的关联，并在dependencies中加入第一新加的词$\\rightarrow$第二新加词，将第二新加词从stack中移除，$\\sigma｜w_i|w_j~~\\beta~~A\\Rightarrow \\sigma|w_j~~\\beta~~A\\cup {r(w_j~w_i)}$\n",
    "3. `RIGHT-ARC`: 将stack中第一新加的词语标记为第二新加的词语的关联，并在dependencies中加入第二新加的词$\\rightarrow$第一新加词，将第一新加词从stack中移除，$\\sigma｜w_i|w_j~~\\beta~~A\\Rightarrow \\sigma|w_i~~\\beta~~A\\cup {r(w_i~w_j)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Neural Network\n",
    "\n",
    "Neural Network的特征向量由由stack中的词以及buffer中的第一个词组成，表示为$W=[w_1,...,w_n]$，每个$0\\leq m \\geq |V|$是词语在词典中的index。NN随即赋予每一个词语一个embedding并合并在一起合成input向量，\n",
    "\n",
    "$$X=[E_{w_1},\\dots,E_{w_m}] \\in {\\Bbb R}^{dm}$$\n",
    "\n",
    "其中$E\\in{\\Bbb R}^{|V|\\times d}$是embedding矩阵\n",
    "\n",
    "然后计算模型的预测结果，\n",
    "\n",
    "$$h=ReLU(xW+b_1)$$\n",
    "\n",
    "$$l=hU+b_2$$\n",
    "\n",
    "$$\\hat{y} = softmax(l)$$\n",
    "\n",
    "其中$h$是神经网络中的hidden layer，$l$为logits，$\\hat{y}$为最后的预测结果。训练NN，使用back-propogation来最小化cross-entropy损失函数，\n",
    "\n",
    "$$J(\\theta)=CE(y,\\hat{y})=-\\displaystyle\\sum_{i=1}^{3} y_ilog \\hat{y_i}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 代码实践\n",
    "\n",
    "对于该算法的代码实践详见[neural transition-based dependency parser code](files/HW3的副本)（目前代码只完成了parser部分，NN部分还没有完成）。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 对基础Recurrent Neural Nework (RNN)结构相关理论的学习"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对基础的RNN结构进行了了解和学习。计划在未来的学习中学习更加复杂的RNN结构，如GRU，LSTM，Multi-layer RNN。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}