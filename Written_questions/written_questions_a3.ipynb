{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 3 Written Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Machine Learning & Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### (a)\n",
    "\n",
    "##### i. \n",
    "\n",
    "Because by keeping track of $m$ at every step, the equation $m \\leftarrow \\beta_1m +(1-\\beta_1) \\nabla_{\\theta} J_{minibatch}(\\theta)$ makes sure that the gradient update at this step is not varying too much from the previous step, while taking a moderate turn to the direction of the new gradient ($\\nabla_{\\theta}J_{minibatch}(\\theta)$). This method can prevent problems of exploding gradient problem, making sure that the updates won't result in a cliff. \n",
    "\n",
    "##### ii. \n",
    "\n",
    "$\\theta$ will get larger update. This helps to increase the updating rate, and thus decrease the training time, while at the same time eliminating the possiblity of gradient exploding since the gradient is normalized by its magnitude. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### (b)\n",
    "\n",
    "##### i.\n",
    "\n",
    "$$E_{p_{drop}}[h_{drop}]_i = h_i$$\n",
    "\n",
    "and,\n",
    "\n",
    "$$E_{p_{drop}}[h_{drop}]_i = (1-p_{drop})\\cdot d|_{d=1} \\cdot h_{drop} + p_{drop} \\cdot d|_{d=0} \\cdot h_{drop}$$\n",
    "\n",
    "and because $h_{drop}=\\gamma d \\odot h$\n",
    "\n",
    "$$E_{p_{drop}}[h_{drop}]_i = (1-p_{drop})\\cdot \\gamma \\cdot h_i = h_i$$\n",
    "\n",
    "thus,\n",
    "\n",
    "$$\\gamma = \\frac{1}{1-p_{drop}}$$\n",
    "\n",
    "##### ii.\n",
    "\n",
    "Because we only apply dropout to the traning set so to avoid overfitting. We don't apply dropout to the test set because we want the test to evaluate how the model is performing in predicting new data. Thus, the more valid data we have for test, the more reliable the test result will be. Then it would be very obvious why we don't apply dropout to the test set, but only the traning set. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Neural Transitions-Based Dependency Parsing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### (a)\n",
    "\n",
    "| Stack | Buffer | New Dependency | Transition |\n",
    "|------|------|------|------|\n",
    "| [ROOT] | [I, parsed, this, sentence, correctly] || Initial Configuration |\n",
    "|[ROOT, I]|[parsed, this, sentence, correctly]||SHIFT|\n",
    "|[ROOT, I, parsed]|[this, sentence, correctly]||SHIFT|\n",
    "|[ROOT, parsed]|[this, sentence, correctly]|parsed$\\rightarrow$I|LEFT-ARC|\n",
    "|[ROOT, parsed, this]|[sentence, correctly]||SHFIT|\n",
    "|[ROOT, parsed, this, sentence]|[correctly]||SHFIT|\n",
    "|[ROOT, parsed, sentence]|[correctly]|sentence$\\rightarrow$this|LEFT-ARC|\n",
    "|[ROOT, parsed]|[correctly]|parsed$\\rightarrow$sentence|RIGHT-ARC|\n",
    "|[ROOT, parsed, correctly]|[]||SHIFT|\n",
    "|[ROOT, parsed]|[]|parsed$\\rightarrow$correctly|RIGHT-ARC|\n",
    "|[ROOT]|[]|ROOT$\\rightarrow$parsed|RIGHT-ARC|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### (b)\n",
    "\n",
    "A sentence which contaning $n$ words will require $2n$ steps to be completely parserd. \n",
    "\n",
    "This is because all $n$ words need to be shifted from buffer to stack, and these opration will take up $n$ steps. And then each word will be dependent of another word (or ROOT), and thus will generate $n$ dependencies, which are equivalent to $n$ steps of LEFT-ARC or RIGHT-ARC. Therefore, taken 2 parts together, the whole parsing process will require $2n$ steps. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### (f)\n",
    "\n",
    "##### i. \n",
    "\n",
    "- **Error Type**: Verb Phrase Attachment Error\n",
    "- **Incorrect Dependency**: wedding $\\rightarrow$ fearing\n",
    "- **Correct Dependency**: heading $\\rightarrow$ fearing\n",
    "\n",
    "##### ii. \n",
    "\n",
    "- **Error Type**: Coordination Attachment Error\n",
    "- **Incorrect Dependency**: makes $\\rightarrow$ rescue\n",
    "- **Correct Dependency**: rush $\\rightarrow$ rescue\n",
    "\n",
    "##### iii. \n",
    "\n",
    "- **Error Type**: Propositional Phrase Attachment Error\n",
    "- **Incorrect Dependency**: named $\\rightarrow$ Midland\n",
    "- **Correct Dependency**: guy $\\rightarrow$ Midland\n",
    "\n",
    "##### iv.\n",
    "\n",
    "- **Error Type**: Modifer Attachment Error\n",
    "- **Incorrect Dependency**: most $\\rightarrow$ element\n",
    "- **Correct Dependency**: most $\\rightarrow$ crucial"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
